Node Embedding
==============

.. note::

    Slides: https://web.stanford.edu/class/cs224w/slides/02-nodeemb.pdf

    Colabs: `Colab 0 <https://colab.research.google.com/drive/10-8W1e_WOX4-YocROm8tHbtmn1frUf2S>`_, `Colab 1 <https://colab.research.google.com/drive/1vvIoEqxGl1naopTZbh4bmCOLEiCxvcQq>`_


Graph representation Learning
-----------------------------

What
^^^^

- **Representation learning**: learning the features automatically, a substitute of feature engineering
- **Graph representation learning**: an automatic process to construct *task-independent* features for graphs.

We want to find a universal function :math:`ENC` (called **Encoder**) such that 

.. note::

    For any graph :math:`G=(V, E)` and each node :math:`v\in V`, :math:`ENC(G, v)\in\mathbb{R}^d`. 

And this process is called **node embedding**.



Why
^^^

1. We don't need to design features for every graph.
2. This general-feature-first-fine-tune-second paradigm proves to be very effective.


How
^^^

Node embedding with encoder + decoder framework

1. Define a similarity metric.
2. Use 2 parametic functions, i.e. encoder and decoder, to complete a roundtrip between the vertice set and the embedding space
3. Optimize the parameters such that the inner products in the embedding space approximates the similarity.


Random walk optimization
------------------------

What
^^^^

.. note::
    Use the occurances in a neighborhood (1-hop information) to define similarity. 
    The neighborhood is generated by some random walk strategy.
    Use boltzman distribution on the inner product to approximates the similarity.


1. Choose a random-walk strategy :math:`R`.
2. Run short fixed-length random walks starting from each node :math:`u` using :math:`R`.
3. Optimize the objective function


Why
^^^


How
^^^

- Negative sampling


Node2vec: biased random walk
----------------------------

Other random walk methods
-------------------------

